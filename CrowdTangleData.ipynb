{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fb9376-fdf6-425a-a7e5-a241d49c611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/md.rafiulbiswas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311a97b-d179-4c0b-bafa-4679249a044a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Arabic stop words from list.txt\n",
    "with open('/content/drive/MyDrive/Facebook Crowdtangle Data/All Zied Download Folder/Renamed by Rafi/Zied Search Term/list.txt', 'r', encoding='utf-8') as file:\n",
    "    arabic_stop_words = set(file.read().splitlines())\n",
    "\n",
    "# Get English stop words from nltk\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Combine Arabic and English stop words\n",
    "stop_words = arabic_stop_words.union(english_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f57b3c-2691-4691-816a-91d9fedbcfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text: remove stop words, punctuation, and non-alphabetic characters\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Check if the value is not a string\n",
    "        return \"\"  # Return an empty string for non-string values\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\u0600-\\u06FF\\s]', '', text)  # Keep Arabic and English letters\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Function to process bigrams and calculate the top 1000\n",
    "def extract_bigrams(df):\n",
    "    # Apply cleaning function to the 'Message' column\n",
    "    df['Cleaned_Message'] = df['Message'].apply(clean_text)\n",
    "\n",
    "    # Combine all cleaned messages into a single string\n",
    "    all_words = \" \".join(df['Cleaned_Message']).split()\n",
    "\n",
    "    # Generate bigrams from the list of words\n",
    "    all_bigrams = list(bigrams(all_words))\n",
    "\n",
    "    # Count bigram frequencies\n",
    "    bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "    # Get the top 500 bigrams\n",
    "    top_500_bigrams = bigram_counts.most_common(500)\n",
    "    return top_500_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840cc6fd-8159-4251-b499-6ecace095825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the CSV files (top-level folder)\n",
    "folder_path = \"/content/drive/MyDrive/Facebook Crowdtangle Data/All Zied Download Folder/Renamed by Rafi/Categorized by Maryam/Economy/Currencies\"  # Replace with the path to your folder\n",
    "output_excel_file = \"data_summary.xlsx\"  # Output Excel file name\n",
    "date_column = \"Post Created Date\"  # Replace with the name of the date column in your CSV files\n",
    "interaction_column = \"Total Interactions (weighted  —  Likes 1x Shares 1x Comments 1x Love 1x Wow 1x Haha 1x Sad 1x Angry 1x Care 1x )\"\n",
    "\n",
    "# Initialize a list to store the results\n",
    "summary_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327115dc-7c98-4615-b56b-47b6e5f225cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CSV file in the root directory (not just subdirectories)\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "\n",
    "        # Read the file\n",
    "        try:\n",
    "            # Read necessary columns\n",
    "            df = pd.read_csv(file_path, usecols=[date_column, interaction_column, 'Message'])\n",
    "\n",
    "            # Convert the date column to datetime\n",
    "            df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "            df = df.dropna(subset=[date_column])  # Drop rows with invalid dates\n",
    "\n",
    "            # Process Total Interactions\n",
    "            # Ensure 'Total Interactions' is numeric, removing commas first\n",
    "            df['Total Interactions'] = df['Total Interactions (weighted  —  Likes 1x Shares 1x Comments 1x Love 1x Wow 1x Haha 1x Sad 1x Angry 1x Care 1x )'].replace(',', '', regex=True)\n",
    "            df['Total Interactions'] = pd.to_numeric(df['Total Interactions'], errors='coerce')\n",
    "\n",
    "            df['Total Interactions'] = df['Total Interactions'].fillna(0)\n",
    "            total_sum = df['Total Interactions'].sum()  # Calculate total interactions\n",
    "\n",
    "            # Calculate the date range\n",
    "            if not df.empty:\n",
    "                min_date = df[date_column].min()\n",
    "                max_date = df[date_column].max()\n",
    "                # Extract the message with the highest interactions\n",
    "                highest_interaction_row = df.loc[df['Total Interactions'].idxmax()]\n",
    "                highest_message = highest_interaction_row['Message']\n",
    "                highest_interactions = highest_interaction_row['Total Interactions']\n",
    "                # Extract top 1000 bigrams\n",
    "                top_1000_bigrams = extract_bigrams(df)\n",
    "                bigram_text = \"\\n\".join([f\"{' '.join(bigram)}: {count}\" for bigram, count in top_1000_bigrams])\n",
    "                # Add summary data\n",
    "                summary_data.append({\n",
    "                    \"File Name\": file_name,\n",
    "                    \"Min Date\": min_date,\n",
    "                    \"Max Date\": max_date,\n",
    "                    \"Total Interactions\": total_sum,\n",
    "                    \"Highest Interaction Message\": highest_message,\n",
    "                    \"Highest Interactions\": highest_interactions,\n",
    "                    \"Top 1000 Bigrams\": bigram_text\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "        # Release memory by deleting the DataFrame\n",
    "        del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5018e-17f8-4550-89b6-aed75364431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverse the folder and its subfolders\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "\n",
    "            # Read the file\n",
    "            try:\n",
    "                # Read necessary columns\n",
    "                df = pd.read_csv(file_path, usecols=[date_column, interaction_column, 'Message'])\n",
    "\n",
    "                # Convert the date column to datetime\n",
    "                df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "                df = df.dropna(subset=[date_column])  # Drop rows with invalid dates\n",
    "\n",
    "                # Process Total Interactions\n",
    "                # Ensure 'Total Interactions' is numeric, removing commas first\n",
    "                df['Total Interactions'] = df['Total Interactions (weighted  —  Likes 1x Shares 1x Comments 1x Love 1x Wow 1x Haha 1x Sad 1x Angry 1x Care 1x )'].replace(',', '', regex=True)\n",
    "                df['Total Interactions'] = pd.to_numeric(df['Total Interactions'], errors='coerce')\n",
    "\n",
    "                df['Total Interactions'] = df['Total Interactions'].fillna(0)\n",
    "                total_sum = df['Total Interactions'].sum()  # Calculate total interactions\n",
    "\n",
    "                # Calculate the date range\n",
    "                if not df.empty:\n",
    "                    min_date = df[date_column].min()\n",
    "                    max_date = df[date_column].max()\n",
    "                    # Extract the message with the highest interactions\n",
    "                    highest_interaction_row = df.loc[df['Total Interactions'].idxmax()]\n",
    "                    highest_message = highest_interaction_row['Message']\n",
    "                    highest_interactions = highest_interaction_row['Total Interactions']\n",
    "                    # Extract top 1000 bigrams\n",
    "                    top_1000_bigrams = extract_bigrams(df)\n",
    "                    bigram_text = \"\\n\".join([f\"{' '.join(bigram)}: {count}\" for bigram, count in top_1000_bigrams])\n",
    "                    # Add summary data\n",
    "                    summary_data.append({\n",
    "                        \"File Name\": file_name,\n",
    "                        \"Min Date\": min_date,\n",
    "                        \"Max Date\": max_date,\n",
    "                        \"Total Interactions\": total_sum,\n",
    "                        \"Highest Interaction Message\": highest_message,\n",
    "                        \"Highest Interactions\": highest_interactions,\n",
    "                        \"Top 1000 Bigrams\": bigram_text\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "            # Release memory by deleting the DataFrame\n",
    "            del df\n",
    "\n",
    "# Write the summary to an Excel file\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_excel(output_excel_file, index=False)\n",
    "print(f\"Summary saved to {output_excel_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
